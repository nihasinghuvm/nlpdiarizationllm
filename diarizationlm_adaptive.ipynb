{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PD5JuW8nF_J4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD5JuW8nF_J4",
    "outputId": "d3a43b53-f528-409d-fed0-e436773f7527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diarizationlm\n",
      "  Downloading diarizationlm-0.1.5-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (2.0.2)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (0.60.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (1.16.3)\n",
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (2.19.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (1.4.0)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (2.8.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from diarizationlm) (4.67.1)\n",
      "Collecting colortimelog (from diarizationlm)\n",
      "  Downloading colortimelog-0.0.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting word-levenshtein (from diarizationlm)\n",
      "  Downloading word_levenshtein-0.0.3.tar.gz (7.7 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->diarizationlm) (6.0.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->diarizationlm) (0.43.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai->diarizationlm) (4.15.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-text->diarizationlm) (2.19.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai->diarizationlm) (3.11)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (3.13.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->diarizationlm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->diarizationlm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->diarizationlm) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->diarizationlm) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->diarizationlm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->diarizationlm) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai->diarizationlm) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->diarizationlm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->diarizationlm) (2.5.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (5.29.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->diarizationlm) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->diarizationlm) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->diarizationlm) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->diarizationlm) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.18.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19.0->tensorflow-text->diarizationlm) (0.1.2)\n",
      "Downloading diarizationlm-0.1.5-py3-none-any.whl (24 kB)\n",
      "Downloading colortimelog-0.0.9-py3-none-any.whl (7.9 kB)\n",
      "Building wheels for collected packages: word-levenshtein\n",
      "  Building wheel for word-levenshtein (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word-levenshtein: filename=word_levenshtein-0.0.3-cp312-cp312-linux_x86_64.whl size=78409 sha256=87a7fec6768d1884e94fc254ee2c5151e16c5881c69529492a80e6bc89ae4577\n",
      "  Stored in directory: /root/.cache/pip/wheels/a5/90/af/c9de0d35b502010a9221b531cce461c295a5e044a29868d016\n",
      "Successfully built word-levenshtein\n",
      "Installing collected packages: colortimelog, word-levenshtein, diarizationlm\n",
      "Successfully installed colortimelog-0.0.9 diarizationlm-0.1.5 word-levenshtein-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install diarizationlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353109ef-75a8-471d-96f1-a44806b4554d",
   "metadata": {
    "id": "353109ef-75a8-471d-96f1-a44806b4554d"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from diarizationlm import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156d23e-b42e-4657-a2b1-b2c730f0b40a",
   "metadata": {
    "id": "e156d23e-b42e-4657-a2b1-b2c730f0b40a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from diarizationlm import utils\n",
    "\n",
    "# Configuration\n",
    "INPUT_FOLDER = \"input_test\"\n",
    "OUTPUT_FOLDER = \"output_adaptive\"\n",
    "MODEL_NAME = \"google/DiarizationLM-8b-Fisher-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdecdd-8a5d-4ba9-b754-8567641ac140",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96bdecdd-8a5d-4ba9-b754-8567641ac140",
    "outputId": "078bd77a-e6c5-40b9-86f3-283c8acb29e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA A100-SXM4-40GB\n",
      "GPU count: 1\n",
      "Current device: 0\n",
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check which GPU you're using\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "\n",
    "    # Check memory usage\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU - this will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZRFLTZXHnCT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4b9c936ea0c14dd8915bbcd851e8b47c",
      "5d1feb898cda4e499f59d9b10b7caf22",
      "fd4e275826be46a98fdb920a2307d0ec",
      "819233e7128d4793a8c2175516a559be",
      "ca30465c6ba44a068a550aa19b4dccb3",
      "dec216c0c773423d8b004b3435987f31",
      "350252e872e245babc631a64c3859091",
      "f53cd412a3414ca2a6e386c47c094090",
      "4a072c6dc2824cd191f818f423cfa140",
      "d447c0083a494b6893feb4ad7139f290",
      "6b2dd57a54c34141a2c4697f2b3273bb"
     ]
    },
    "id": "mZRFLTZXHnCT",
    "outputId": "e4286860-5344-4ad7-e5a4-4bbc4b229c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DiarizationLM model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9c936ea0c14dd8915bbcd851e8b47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 JSON files.\n",
      "\n",
      "\n",
      "Processing SBC017.json\n",
      "Adaptive chunking created 45 chunks.\n",
      "Chunk 1/45 (size 200)\n",
      "   → Processing 200 words...\n",
      "Chunk 2/45 (size 53)\n",
      "   → Processing 53 words...\n",
      "Chunk 3/45 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 4/45 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 5/45 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 6/45 (size 67)\n",
      "   → Processing 67 words...\n",
      "Chunk 7/45 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 8/45 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 9/45 (size 126)\n",
      "   → Processing 126 words...\n",
      "Chunk 10/45 (size 88)\n",
      "   → Processing 88 words...\n",
      "Chunk 11/45 (size 60)\n",
      "   → Processing 60 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 12/45 (size 32)\n",
      "   → Processing 32 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 13/45 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 14/45 (size 162)\n",
      "   → Processing 162 words...\n",
      "Chunk 15/45 (size 70)\n",
      "   → Processing 70 words...\n",
      "Chunk 16/45 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 17/45 (size 329)\n",
      "   → Processing 329 words...\n",
      "Chunk 18/45 (size 101)\n",
      "   → Processing 101 words...\n",
      "Chunk 19/45 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 20/45 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 21/45 (size 46)\n",
      "   → Processing 46 words...\n",
      "Chunk 22/45 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 23/45 (size 213)\n",
      "   → Processing 213 words...\n",
      "Chunk 24/45 (size 198)\n",
      "   → Processing 198 words...\n",
      "Chunk 25/45 (size 71)\n",
      "   → Processing 71 words...\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Chunk 26/45 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 27/45 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 28/45 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 29/45 (size 68)\n",
      "   → Processing 68 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 30/45 (size 70)\n",
      "   → Processing 70 words...\n",
      "Chunk 31/45 (size 394)\n",
      "   → Processing 394 words...\n",
      "Chunk 32/45 (size 66)\n",
      "   → Processing 66 words...\n",
      "Chunk 33/45 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 34/45 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 35/45 (size 45)\n",
      "   → Processing 45 words...\n",
      "Chunk 36/45 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 37/45 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 38/45 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 39/45 (size 59)\n",
      "   → Processing 59 words...\n",
      "Chunk 40/45 (size 45)\n",
      "   → Processing 45 words...\n",
      "Chunk 41/45 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 42/45 (size 128)\n",
      "   → Processing 128 words...\n",
      "Chunk 43/45 (size 139)\n",
      "   → Processing 139 words...\n",
      "Chunk 44/45 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 45/45 (size 68)\n",
      "   → Processing 68 words...\n",
      "   Total speaker label changes: 113\n",
      "Saved → output_adaptive/SBC017.json\n",
      "\n",
      "Processing SBC034.json\n",
      "Adaptive chunking created 74 chunks.\n",
      "Chunk 1/74 (size 320)\n",
      "   → Processing 320 words...\n",
      "Chunk 2/74 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 3/74 (size 211)\n",
      "   → Processing 211 words...\n",
      "Chunk 4/74 (size 57)\n",
      "   → Processing 57 words...\n",
      "Chunk 5/74 (size 90)\n",
      "   → Processing 90 words...\n",
      "Chunk 6/74 (size 27)\n",
      "   → Processing 27 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 7/74 (size 329)\n",
      "   → Processing 329 words...\n",
      "Chunk 8/74 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 9/74 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 10/74 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 11/74 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 12/74 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 13/74 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 14/74 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 15/74 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 16/74 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 17/74 (size 67)\n",
      "   → Processing 67 words...\n",
      "Chunk 18/74 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 19/74 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 20/74 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 21/74 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 22/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 23/74 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 24/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 25/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 26/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 27/74 (size 30)\n",
      "   → Processing 30 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 28/74 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 29/74 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 30/74 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 31/74 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 32/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 33/74 (size 87)\n",
      "   → Processing 87 words...\n",
      "Chunk 34/74 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 35/74 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 36/74 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 37/74 (size 27)\n",
      "   → Processing 27 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 38/74 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 39/74 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 40/74 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 41/74 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 42/74 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 43/74 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 44/74 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 45/74 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 46/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 47/74 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 48/74 (size 56)\n",
      "   → Processing 56 words...\n",
      "Chunk 49/74 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 50/74 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 51/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 52/74 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 53/74 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 54/74 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 55/74 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 56/74 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 57/74 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 58/74 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 59/74 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 60/74 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 61/74 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 62/74 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 63/74 (size 67)\n",
      "   → Processing 67 words...\n",
      "Chunk 64/74 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 65/74 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 66/74 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 67/74 (size 142)\n",
      "   → Processing 142 words...\n",
      "Chunk 68/74 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 69/74 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 70/74 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 71/74 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 72/74 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 73/74 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 74/74 (size 6)\n",
      "   → Processing 6 words...\n",
      "   Total speaker label changes: 37\n",
      "Saved → output_adaptive/SBC034.json\n",
      "\n",
      "Processing SBC028.json\n",
      "Adaptive chunking created 40 chunks.\n",
      "Chunk 1/40 (size 1)\n",
      "   → Processing 1 words...\n",
      "Chunk 2/40 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 3/40 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 4/40 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 5/40 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 6/40 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 7/40 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 8/40 (size 922)\n",
      "   → Processing 922 words...\n",
      "Chunk 9/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 10/40 (size 574)\n",
      "   → Processing 574 words...\n",
      "Chunk 11/40 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 12/40 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 13/40 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 14/40 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 15/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 16/40 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 17/40 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 18/40 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 19/40 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 20/40 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 21/40 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 22/40 (size 205)\n",
      "   → Processing 205 words...\n",
      "Chunk 23/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 24/40 (size 145)\n",
      "   → Processing 145 words...\n",
      "Chunk 25/40 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 26/40 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 27/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 28/40 (size 67)\n",
      "   → Processing 67 words...\n",
      "Chunk 29/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 30/40 (size 243)\n",
      "   → Processing 243 words...\n",
      "Skipping meaningless speaker token: <speaker:2>.>\n",
      "Chunk 31/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 32/40 (size 1217)\n",
      "   → Processing 1217 words...\n",
      "Skipping meaningless speaker token: <speaker:2>?>\n",
      "Chunk 33/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 34/40 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 35/40 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 36/40 (size 95)\n",
      "   → Processing 95 words...\n",
      "Chunk 37/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 38/40 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 39/40 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 40/40 (size 190)\n",
      "   → Processing 190 words...\n",
      "   Total speaker label changes: 771\n",
      "Saved → output_adaptive/SBC028.json\n",
      "\n",
      "Processing SBC005.json\n",
      "Adaptive chunking created 120 chunks.\n",
      "Chunk 1/120 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 2/120 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 3/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 4/120 (size 191)\n",
      "   → Processing 191 words...\n",
      "Chunk 5/120 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 6/120 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 7/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 8/120 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 9/120 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 10/120 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 11/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 12/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 13/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 14/120 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 15/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 16/120 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 17/120 (size 51)\n",
      "   → Processing 51 words...\n",
      "Chunk 18/120 (size 38)\n",
      "   → Processing 38 words...\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Chunk 19/120 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 20/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 21/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 22/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 23/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 24/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 25/120 (size 50)\n",
      "   → Processing 50 words...\n",
      "Chunk 26/120 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 27/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 28/120 (size 108)\n",
      "   → Processing 108 words...\n",
      "Chunk 29/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 30/120 (size 77)\n",
      "   → Processing 77 words...\n",
      "Chunk 31/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 32/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 33/120 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 34/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 35/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 36/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 37/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 38/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 39/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 40/120 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 41/120 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 42/120 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 43/120 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 44/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 45/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 46/120 (size 74)\n",
      "   → Processing 74 words...\n",
      "Chunk 47/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 48/120 (size 20)\n",
      "   → Processing 20 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 49/120 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 50/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 51/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 52/120 (size 45)\n",
      "   → Processing 45 words...\n",
      "Chunk 53/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 54/120 (size 70)\n",
      "   → Processing 70 words...\n",
      "Chunk 55/120 (size 35)\n",
      "   → Processing 35 words...\n",
      "Chunk 56/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 57/120 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 58/120 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 59/120 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 60/120 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 61/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 62/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 63/120 (size 87)\n",
      "   → Processing 87 words...\n",
      "Chunk 64/120 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 65/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 66/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 67/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 68/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 69/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 70/120 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 71/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 72/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 73/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 74/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 75/120 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 76/120 (size 87)\n",
      "   → Processing 87 words...\n",
      "Chunk 77/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 78/120 (size 141)\n",
      "   → Processing 141 words...\n",
      "Chunk 79/120 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 80/120 (size 86)\n",
      "   → Processing 86 words...\n",
      "Chunk 81/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 82/120 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 83/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 84/120 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 85/120 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 86/120 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 87/120 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 88/120 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 89/120 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 90/120 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 91/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 92/120 (size 86)\n",
      "   → Processing 86 words...\n",
      "Chunk 93/120 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 94/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 95/120 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 96/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 97/120 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 98/120 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 99/120 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 100/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 101/120 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 102/120 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 103/120 (size 28)\n",
      "   → Processing 28 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 104/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 105/120 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 106/120 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 107/120 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 108/120 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 109/120 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 110/120 (size 49)\n",
      "   → Processing 49 words...\n",
      "Chunk 111/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 112/120 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 113/120 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 114/120 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 115/120 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 116/120 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 117/120 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 118/120 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 119/120 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 120/120 (size 62)\n",
      "   → Processing 62 words...\n",
      "   Total speaker label changes: 43\n",
      "Saved → output_adaptive/SBC005.json\n",
      "\n",
      "Processing SBC047.json\n",
      "Adaptive chunking created 98 chunks.\n",
      "Chunk 1/98 (size 234)\n",
      "   → Processing 234 words...\n",
      "Chunk 2/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 3/98 (size 123)\n",
      "   → Processing 123 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 4/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 5/98 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 6/98 (size 30)\n",
      "   → Processing 30 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 7/98 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 8/98 (size 26)\n",
      "   → Processing 26 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 9/98 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 10/98 (size 121)\n",
      "   → Processing 121 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 11/98 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 12/98 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 13/98 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 14/98 (size 52)\n",
      "   → Processing 52 words...\n",
      "Chunk 15/98 (size 29)\n",
      "   → Processing 29 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 16/98 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 17/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 18/98 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 19/98 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 20/98 (size 95)\n",
      "   → Processing 95 words...\n",
      "Chunk 21/98 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 22/98 (size 100)\n",
      "   → Processing 100 words...\n",
      "Chunk 23/98 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 24/98 (size 124)\n",
      "   → Processing 124 words...\n",
      "Chunk 25/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 26/98 (size 161)\n",
      "   → Processing 161 words...\n",
      "Chunk 27/98 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 28/98 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 29/98 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 30/98 (size 152)\n",
      "   → Processing 152 words...\n",
      "Chunk 31/98 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 32/98 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 33/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 34/98 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 35/98 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 36/98 (size 154)\n",
      "   → Processing 154 words...\n",
      "Chunk 37/98 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 38/98 (size 61)\n",
      "   → Processing 61 words...\n",
      "Chunk 39/98 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 40/98 (size 91)\n",
      "   → Processing 91 words...\n",
      "Chunk 41/98 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 42/98 (size 77)\n",
      "   → Processing 77 words...\n",
      "Chunk 43/98 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 44/98 (size 161)\n",
      "   → Processing 161 words...\n",
      "Chunk 45/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 46/98 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 47/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 48/98 (size 175)\n",
      "   → Processing 175 words...\n",
      "Chunk 49/98 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 50/98 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 51/98 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 52/98 (size 71)\n",
      "   → Processing 71 words...\n",
      "Chunk 53/98 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 54/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 55/98 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 56/98 (size 23)\n",
      "   → Processing 23 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 57/98 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 58/98 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 59/98 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 60/98 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 61/98 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 62/98 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 63/98 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 64/98 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 65/98 (size 29)\n",
      "   → Processing 29 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 66/98 (size 61)\n",
      "   → Processing 61 words...\n",
      "Chunk 67/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 68/98 (size 101)\n",
      "   → Processing 101 words...\n",
      "Chunk 69/98 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 70/98 (size 118)\n",
      "   → Processing 118 words...\n",
      "Chunk 71/98 (size 70)\n",
      "   → Processing 70 words...\n",
      "Chunk 72/98 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 73/98 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 74/98 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 75/98 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 76/98 (size 77)\n",
      "   → Processing 77 words...\n",
      "Chunk 77/98 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 78/98 (size 139)\n",
      "   → Processing 139 words...\n",
      "Chunk 79/98 (size 50)\n",
      "   → Processing 50 words...\n",
      "Chunk 80/98 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 81/98 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 82/98 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 83/98 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 84/98 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 85/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 86/98 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 87/98 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 88/98 (size 61)\n",
      "   → Processing 61 words...\n",
      "Chunk 89/98 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 90/98 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 91/98 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 92/98 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 93/98 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 94/98 (size 180)\n",
      "   → Processing 180 words...\n",
      "Chunk 95/98 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 96/98 (size 127)\n",
      "   → Processing 127 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 97/98 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 98/98 (size 7)\n",
      "   → Processing 7 words...\n",
      "   Total speaker label changes: 131\n",
      "Saved → output_adaptive/SBC047.json\n",
      "\n",
      "Processing SBC009.json\n",
      "Adaptive chunking created 155 chunks.\n",
      "Chunk 1/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 2/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 3/155 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 4/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 5/155 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 6/155 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 7/155 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 8/155 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 9/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 10/155 (size 27)\n",
      "   → Processing 27 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 11/155 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 12/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 13/155 (size 25)\n",
      "   → Processing 25 words...\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Chunk 14/155 (size 53)\n",
      "   → Processing 53 words...\n",
      "Chunk 15/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 16/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 17/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 18/155 (size 103)\n",
      "   → Processing 103 words...\n",
      "Chunk 19/155 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 20/155 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 21/155 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 22/155 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 23/155 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 24/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 25/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 26/155 (size 35)\n",
      "   → Processing 35 words...\n",
      "Chunk 27/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 28/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 29/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 30/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 31/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 32/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 33/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 34/155 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 35/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 36/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 37/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 38/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 39/155 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 40/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 41/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 42/155 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 43/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 44/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 45/155 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 46/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 47/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 48/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 49/155 (size 46)\n",
      "   → Processing 46 words...\n",
      "Chunk 50/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 51/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 52/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 53/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 54/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 55/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 56/155 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 57/155 (size 83)\n",
      "   → Processing 83 words...\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Skipping meaningless speaker token: <speaker:1>.>\n",
      "Chunk 58/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 59/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 60/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 61/155 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 62/155 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 63/155 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 64/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 65/155 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 66/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 67/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 68/155 (size 82)\n",
      "   → Processing 82 words...\n",
      "Chunk 69/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 70/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 71/155 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 72/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 73/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 74/155 (size 21)\n",
      "   → Processing 21 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 75/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 76/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 77/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 78/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 79/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 80/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 81/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 82/155 (size 49)\n",
      "   → Processing 49 words...\n",
      "Chunk 83/155 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 84/155 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 85/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 86/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 87/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 88/155 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 89/155 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 90/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 91/155 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 92/155 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 93/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 94/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 95/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 96/155 (size 28)\n",
      "   → Processing 28 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 97/155 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 98/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 99/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 100/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 101/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 102/155 (size 82)\n",
      "   → Processing 82 words...\n",
      "Chunk 103/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 104/155 (size 54)\n",
      "   → Processing 54 words...\n",
      "Chunk 105/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 106/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 107/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 108/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 109/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 110/155 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 111/155 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 112/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 113/155 (size 50)\n",
      "   → Processing 50 words...\n",
      "Chunk 114/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 115/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 116/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 117/155 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 118/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 119/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 120/155 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 121/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 122/155 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 123/155 (size 50)\n",
      "   → Processing 50 words...\n",
      "Chunk 124/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 125/155 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 126/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 127/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 128/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 129/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 130/155 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 131/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 132/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 133/155 (size 21)\n",
      "   → Processing 21 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 134/155 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 135/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 136/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 137/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 138/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 139/155 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 140/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 141/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 142/155 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 143/155 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 144/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 145/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 146/155 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 147/155 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 148/155 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 149/155 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 150/155 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 151/155 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 152/155 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 153/155 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 154/155 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 155/155 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "   Total speaker label changes: 49\n",
      "Saved → output_adaptive/SBC009.json\n",
      "\n",
      "Processing SBC022.json\n",
      "Adaptive chunking created 31 chunks.\n",
      "Chunk 1/31 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 2/31 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 3/31 (size 93)\n",
      "   → Processing 93 words...\n",
      "Chunk 4/31 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 5/31 (size 131)\n",
      "   → Processing 131 words...\n",
      "Chunk 6/31 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 7/31 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 8/31 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 9/31 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 10/31 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 11/31 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 12/31 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 13/31 (size 89)\n",
      "   → Processing 89 words...\n",
      "Chunk 14/31 (size 175)\n",
      "   → Processing 175 words...\n",
      "Chunk 15/31 (size 215)\n",
      "   → Processing 215 words...\n",
      "Chunk 16/31 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 17/31 (size 316)\n",
      "   → Processing 316 words...\n",
      "Chunk 18/31 (size 146)\n",
      "   → Processing 146 words...\n",
      "Chunk 19/31 (size 122)\n",
      "   → Processing 122 words...\n",
      "Chunk 20/31 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 21/31 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 22/31 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 23/31 (size 46)\n",
      "   → Processing 46 words...\n",
      "Chunk 24/31 (size 48)\n",
      "   → Processing 48 words...\n",
      "Chunk 25/31 (size 101)\n",
      "   → Processing 101 words...\n",
      "Chunk 26/31 (size 80)\n",
      "   → Processing 80 words...\n",
      "Chunk 27/31 (size 72)\n",
      "   → Processing 72 words...\n",
      "Chunk 28/31 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 29/31 (size 144)\n",
      "   → Processing 144 words...\n",
      "Chunk 30/31 (size 62)\n",
      "   → Processing 62 words...\n",
      "Chunk 31/31 (size 10)\n",
      "   → Processing 10 words...\n",
      "   Total speaker label changes: 9\n",
      "Saved → output_adaptive/SBC022.json\n",
      "\n",
      "Processing SBC043.json\n",
      "Adaptive chunking created 129 chunks.\n",
      "Chunk 1/129 (size 458)\n",
      "   → Processing 458 words...\n",
      "Chunk 2/129 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 3/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 4/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 5/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 6/129 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 7/129 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 8/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 9/129 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 10/129 (size 75)\n",
      "   → Processing 75 words...\n",
      "Chunk 11/129 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 12/129 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 13/129 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 14/129 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 15/129 (size 75)\n",
      "   → Processing 75 words...\n",
      "Chunk 16/129 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 17/129 (size 61)\n",
      "   → Processing 61 words...\n",
      "Chunk 18/129 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 19/129 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 20/129 (size 194)\n",
      "   → Processing 194 words...\n",
      "Chunk 21/129 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 22/129 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 23/129 (size 202)\n",
      "   → Processing 202 words...\n",
      "Chunk 24/129 (size 218)\n",
      "   → Processing 218 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 25/129 (size 103)\n",
      "   → Processing 103 words...\n",
      "Chunk 26/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 27/129 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 28/129 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 29/129 (size 62)\n",
      "   → Processing 62 words...\n",
      "Chunk 30/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 31/129 (size 182)\n",
      "   → Processing 182 words...\n",
      "Chunk 32/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 33/129 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 34/129 (size 302)\n",
      "   → Processing 302 words...\n",
      "Chunk 35/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 36/129 (size 183)\n",
      "   → Processing 183 words...\n",
      "Chunk 37/129 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 38/129 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 39/129 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 40/129 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 41/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 42/129 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 43/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 44/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 45/129 (size 35)\n",
      "   → Processing 35 words...\n",
      "Chunk 46/129 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 47/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 48/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 49/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 50/129 (size 23)\n",
      "   → Processing 23 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 51/129 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 52/129 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 53/129 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 54/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 55/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 56/129 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 57/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 58/129 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 59/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 60/129 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 61/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 62/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 63/129 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 64/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 65/129 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 66/129 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 67/129 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 68/129 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 69/129 (size 48)\n",
      "   → Processing 48 words...\n",
      "Skipping meaningless speaker token: <speaker:2>.>\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 70/129 (size 101)\n",
      "   → Processing 101 words...\n",
      "Chunk 71/129 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 72/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 73/129 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 74/129 (size 52)\n",
      "   → Processing 52 words...\n",
      "Chunk 75/129 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 76/129 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 77/129 (size 185)\n",
      "   → Processing 185 words...\n",
      "Chunk 78/129 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 79/129 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 80/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 81/129 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 82/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 83/129 (size 133)\n",
      "   → Processing 133 words...\n",
      "Chunk 84/129 (size 93)\n",
      "   → Processing 93 words...\n",
      "Chunk 85/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 86/129 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 87/129 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 88/129 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 89/129 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 90/129 (size 18)\n",
      "   → Processing 18 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 91/129 (size 286)\n",
      "   → Processing 286 words...\n",
      "Chunk 92/129 (size 60)\n",
      "   → Processing 60 words...\n",
      "Chunk 93/129 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 94/129 (size 85)\n",
      "   → Processing 85 words...\n",
      "Chunk 95/129 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 96/129 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 97/129 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 98/129 (size 26)\n",
      "   → Processing 26 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 99/129 (size 25)\n",
      "   → Processing 25 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 100/129 (size 254)\n",
      "   → Processing 254 words...\n",
      "Chunk 101/129 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 102/129 (size 54)\n",
      "   → Processing 54 words...\n",
      "Chunk 103/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 104/129 (size 118)\n",
      "   → Processing 118 words...\n",
      "Chunk 105/129 (size 59)\n",
      "   → Processing 59 words...\n",
      "Chunk 106/129 (size 70)\n",
      "   → Processing 70 words...\n",
      "Chunk 107/129 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 108/129 (size 72)\n",
      "   → Processing 72 words...\n",
      "Chunk 109/129 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 110/129 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 111/129 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 112/129 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 113/129 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 114/129 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 115/129 (size 18)\n",
      "   → Processing 18 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 116/129 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 117/129 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 118/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 119/129 (size 27)\n",
      "   → Processing 27 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 120/129 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 121/129 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 122/129 (size 105)\n",
      "   → Processing 105 words...\n",
      "Chunk 123/129 (size 109)\n",
      "   → Processing 109 words...\n",
      "Chunk 124/129 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 125/129 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 126/129 (size 109)\n",
      "   → Processing 109 words...\n",
      "Chunk 127/129 (size 105)\n",
      "   → Processing 105 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 128/129 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 129/129 (size 10)\n",
      "   → Processing 10 words...\n",
      "   Total speaker label changes: 104\n",
      "Saved → output_adaptive/SBC043.json\n",
      "\n",
      "Processing SBC024.json\n",
      "Adaptive chunking created 111 chunks.\n",
      "Chunk 1/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 2/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 3/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 4/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 5/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 6/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 7/111 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 8/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 9/111 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 10/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 11/111 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 12/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 13/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 14/111 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 15/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 16/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 17/111 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 18/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 19/111 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 20/111 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 21/111 (size 20)\n",
      "   → Processing 20 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 22/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 23/111 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 24/111 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 25/111 (size 86)\n",
      "   → Processing 86 words...\n",
      "Chunk 26/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 27/111 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 28/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 29/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 30/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 31/111 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 32/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 33/111 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 34/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 35/111 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 36/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 37/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 38/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 39/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 40/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 41/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 42/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 43/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 44/111 (size 20)\n",
      "   → Processing 20 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 45/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 46/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 47/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 48/111 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 49/111 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 50/111 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 51/111 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 52/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 53/111 (size 17)\n",
      "   → Processing 17 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 54/111 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 55/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 56/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 57/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 58/111 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 59/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 60/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 61/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 62/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 63/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 64/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 65/111 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 66/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 67/111 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 68/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 69/111 (size 21)\n",
      "   → Processing 21 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 70/111 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 71/111 (size 32)\n",
      "   → Processing 32 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 72/111 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 73/111 (size 89)\n",
      "   → Processing 89 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 74/111 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 75/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 76/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 77/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 78/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 79/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 80/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 81/111 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 82/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 83/111 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 84/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 85/111 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 86/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 87/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 88/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 89/111 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 90/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 91/111 (size 35)\n",
      "   → Processing 35 words...\n",
      "Chunk 92/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 93/111 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 94/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 95/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 96/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 97/111 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 98/111 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 99/111 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 100/111 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 101/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 102/111 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 103/111 (size 827)\n",
      "   → Processing 827 words...\n",
      "Chunk 104/111 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 105/111 (size 56)\n",
      "   → Processing 56 words...\n",
      "Chunk 106/111 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 107/111 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 108/111 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 109/111 (size 122)\n",
      "   → Processing 122 words...\n",
      "Chunk 110/111 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 111/111 (size 68)\n",
      "   → Processing 68 words...\n",
      "   Total speaker label changes: 67\n",
      "Saved → output_adaptive/SBC024.json\n",
      "\n",
      "Processing SBC060.json\n",
      "Adaptive chunking created 36 chunks.\n",
      "Chunk 1/36 (size 897)\n",
      "   → Processing 897 words...\n",
      "Chunk 2/36 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 3/36 (size 95)\n",
      "   → Processing 95 words...\n",
      "Chunk 4/36 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 5/36 (size 236)\n",
      "   → Processing 236 words...\n",
      "Chunk 6/36 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 7/36 (size 427)\n",
      "   → Processing 427 words...\n",
      "Chunk 8/36 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 9/36 (size 206)\n",
      "   → Processing 206 words...\n",
      "Chunk 10/36 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 11/36 (size 20)\n",
      "   → Processing 20 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 12/36 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 13/36 (size 65)\n",
      "   → Processing 65 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 14/36 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 15/36 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 16/36 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 17/36 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 18/36 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 19/36 (size 102)\n",
      "   → Processing 102 words...\n",
      "Chunk 20/36 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 21/36 (size 21)\n",
      "   → Processing 21 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 22/36 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 23/36 (size 71)\n",
      "   → Processing 71 words...\n",
      "Chunk 24/36 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 25/36 (size 71)\n",
      "   → Processing 71 words...\n",
      "Chunk 26/36 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 27/36 (size 239)\n",
      "   → Processing 239 words...\n",
      "Chunk 28/36 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 29/36 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 30/36 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 31/36 (size 112)\n",
      "   → Processing 112 words...\n",
      "Chunk 32/36 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 33/36 (size 329)\n",
      "   → Processing 329 words...\n",
      "Chunk 34/36 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 35/36 (size 46)\n",
      "   → Processing 46 words...\n",
      "Chunk 36/36 (size 29)\n",
      "   → Processing 29 words...\n",
      "   Total speaker label changes: 14\n",
      "Saved → output_adaptive/SBC060.json\n",
      "\n",
      "Processing SBC044.json\n",
      "Adaptive chunking created 68 chunks.\n",
      "Chunk 1/68 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 2/68 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 3/68 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 4/68 (size 297)\n",
      "   → Processing 297 words...\n",
      "Chunk 5/68 (size 107)\n",
      "   → Processing 107 words...\n",
      "Chunk 6/68 (size 196)\n",
      "   → Processing 196 words...\n",
      "Chunk 7/68 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 8/68 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 9/68 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 10/68 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 11/68 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 12/68 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 13/68 (size 27)\n",
      "   → Processing 27 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 14/68 (size 60)\n",
      "   → Processing 60 words...\n",
      "Chunk 15/68 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 16/68 (size 242)\n",
      "   → Processing 242 words...\n",
      "Chunk 17/68 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 18/68 (size 195)\n",
      "   → Processing 195 words...\n",
      "Chunk 19/68 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 20/68 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 21/68 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 22/68 (size 74)\n",
      "   → Processing 74 words...\n",
      "Chunk 23/68 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 24/68 (size 120)\n",
      "   → Processing 120 words...\n",
      "Chunk 25/68 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 26/68 (size 207)\n",
      "   → Processing 207 words...\n",
      "Chunk 27/68 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 28/68 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 29/68 (size 126)\n",
      "   → Processing 126 words...\n",
      "Chunk 30/68 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 31/68 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 32/68 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 33/68 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 34/68 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 35/68 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 36/68 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 37/68 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 38/68 (size 216)\n",
      "   → Processing 216 words...\n",
      "Chunk 39/68 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 40/68 (size 320)\n",
      "   → Processing 320 words...\n",
      "Chunk 41/68 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 42/68 (size 179)\n",
      "   → Processing 179 words...\n",
      "Chunk 43/68 (size 117)\n",
      "   → Processing 117 words...\n",
      "Chunk 44/68 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 45/68 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 46/68 (size 368)\n",
      "   → Processing 368 words...\n",
      "Skipping meaningless speaker token: <speaker:2>?>\n",
      "Chunk 47/68 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 48/68 (size 42)\n",
      "   → Processing 42 words...\n",
      "Chunk 49/68 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 50/68 (size 194)\n",
      "   → Processing 194 words...\n",
      "Chunk 51/68 (size 20)\n",
      "   → Processing 20 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 52/68 (size 78)\n",
      "   → Processing 78 words...\n",
      "Chunk 53/68 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 54/68 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 55/68 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 56/68 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 57/68 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 58/68 (size 161)\n",
      "   → Processing 161 words...\n",
      "Chunk 59/68 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 60/68 (size 316)\n",
      "   → Processing 316 words...\n",
      "Chunk 61/68 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 62/68 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 63/68 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 64/68 (size 115)\n",
      "   → Processing 115 words...\n",
      "Chunk 65/68 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 66/68 (size 235)\n",
      "   → Processing 235 words...\n",
      "Skipping meaningless speaker token: <speaker:2>.>\n",
      "Chunk 67/68 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 68/68 (size 1199)\n",
      "   → Processing 1199 words...\n",
      "   Total speaker label changes: 77\n",
      "Saved → output_adaptive/SBC044.json\n",
      "\n",
      "Processing SBC058.json\n",
      "Adaptive chunking created 21 chunks.\n",
      "Chunk 1/21 (size 246)\n",
      "   → Processing 246 words...\n",
      "Chunk 2/21 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 3/21 (size 274)\n",
      "   → Processing 274 words...\n",
      "Chunk 4/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 5/21 (size 352)\n",
      "   → Processing 352 words...\n",
      "Chunk 6/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 7/21 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 8/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 9/21 (size 952)\n",
      "   → Processing 952 words...\n",
      "Chunk 10/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 11/21 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 12/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 13/21 (size 157)\n",
      "   → Processing 157 words...\n",
      "Chunk 14/21 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 15/21 (size 115)\n",
      "   → Processing 115 words...\n",
      "Chunk 16/21 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 17/21 (size 391)\n",
      "   → Processing 391 words...\n",
      "Chunk 18/21 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 19/21 (size 254)\n",
      "   → Processing 254 words...\n",
      "Chunk 20/21 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 21/21 (size 54)\n",
      "   → Processing 54 words...\n",
      "   Total speaker label changes: 38\n",
      "Saved → output_adaptive/SBC058.json\n",
      "\n",
      "Processing SBC041.json\n",
      "Adaptive chunking created 29 chunks.\n",
      "Chunk 1/29 (size 187)\n",
      "   → Processing 187 words...\n",
      "Chunk 2/29 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 3/29 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 4/29 (size 23)\n",
      "   → Processing 23 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 5/29 (size 159)\n",
      "   → Processing 159 words...\n",
      "Chunk 6/29 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 7/29 (size 161)\n",
      "   → Processing 161 words...\n",
      "Chunk 8/29 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 9/29 (size 252)\n",
      "   → Processing 252 words...\n",
      "Chunk 10/29 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 11/29 (size 89)\n",
      "   → Processing 89 words...\n",
      "Chunk 12/29 (size 21)\n",
      "   → Processing 21 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 13/29 (size 112)\n",
      "   → Processing 112 words...\n",
      "Chunk 14/29 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 15/29 (size 108)\n",
      "   → Processing 108 words...\n",
      "Chunk 16/29 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 17/29 (size 219)\n",
      "   → Processing 219 words...\n",
      "Chunk 18/29 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 19/29 (size 91)\n",
      "   → Processing 91 words...\n",
      "Chunk 20/29 (size 144)\n",
      "   → Processing 144 words...\n",
      "Chunk 21/29 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 22/29 (size 82)\n",
      "   → Processing 82 words...\n",
      "Chunk 23/29 (size 157)\n",
      "   → Processing 157 words...\n",
      "Chunk 24/29 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 25/29 (size 65)\n",
      "   → Processing 65 words...\n",
      "Chunk 26/29 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 27/29 (size 329)\n",
      "   → Processing 329 words...\n",
      "Chunk 28/29 (size 59)\n",
      "   → Processing 59 words...\n",
      "Chunk 29/29 (size 202)\n",
      "   → Processing 202 words...\n",
      "   Total speaker label changes: 18\n",
      "Saved → output_adaptive/SBC041.json\n",
      "\n",
      "Processing SBC029.json\n",
      "Adaptive chunking created 205 chunks.\n",
      "Chunk 1/205 (size 2)\n",
      "   → Processing 2 words...\n",
      "Chunk 2/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 3/205 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 4/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 5/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 6/205 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 7/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 8/205 (size 28)\n",
      "   → Processing 28 words...\n",
      "Chunk 9/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 10/205 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 11/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 12/205 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 13/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 14/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 15/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 16/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 17/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 18/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 19/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 20/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 21/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 22/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 23/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 24/205 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 25/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 26/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 27/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 28/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 29/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 30/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 31/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 32/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 33/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 34/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 35/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 36/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 37/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 38/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 39/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 40/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 41/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 42/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 43/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 44/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 45/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 46/205 (size 41)\n",
      "   → Processing 41 words...\n",
      "Chunk 47/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 48/205 (size 49)\n",
      "   → Processing 49 words...\n",
      "Chunk 49/205 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 50/205 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 51/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 52/205 (size 28)\n",
      "   → Processing 28 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 53/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 54/205 (size 61)\n",
      "   → Processing 61 words...\n",
      "Chunk 55/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 56/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 57/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 58/205 (size 52)\n",
      "   → Processing 52 words...\n",
      "Chunk 59/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 60/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 61/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 62/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 63/205 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 64/205 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 65/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 66/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 67/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 68/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 69/205 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 70/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 71/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 72/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 73/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 74/205 (size 49)\n",
      "   → Processing 49 words...\n",
      "Chunk 75/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 76/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 77/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 78/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 79/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 80/205 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 81/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 82/205 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 83/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 84/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 85/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 86/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 87/205 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 88/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 89/205 (size 135)\n",
      "   → Processing 135 words...\n",
      "Chunk 90/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 91/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 92/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 93/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 94/205 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 95/205 (size 43)\n",
      "   → Processing 43 words...\n",
      "Skipping meaningless speaker token: <speaker:2>...goes>\n",
      "Chunk 96/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 97/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 98/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 99/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 100/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 101/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 102/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 103/205 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 104/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 105/205 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 106/205 (size 122)\n",
      "   → Processing 122 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 107/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 108/205 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 109/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 110/205 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 111/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 112/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 113/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 114/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 115/205 (size 57)\n",
      "   → Processing 57 words...\n",
      "Chunk 116/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 117/205 (size 135)\n",
      "   → Processing 135 words...\n",
      "Chunk 118/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 119/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 120/205 (size 60)\n",
      "   → Processing 60 words...\n",
      "Chunk 121/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 122/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 123/205 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 124/205 (size 45)\n",
      "   → Processing 45 words...\n",
      "Chunk 125/205 (size 161)\n",
      "   → Processing 161 words...\n",
      "Chunk 126/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 127/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 128/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 129/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 130/205 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 131/205 (size 131)\n",
      "   → Processing 131 words...\n",
      "Chunk 132/205 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 133/205 (size 49)\n",
      "   → Processing 49 words...\n",
      "Chunk 134/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 135/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 136/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 137/205 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 138/205 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 139/205 (size 51)\n",
      "   → Processing 51 words...\n",
      "Chunk 140/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 141/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 142/205 (size 37)\n",
      "   → Processing 37 words...\n",
      "Chunk 143/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 144/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 145/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 146/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 147/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 148/205 (size 53)\n",
      "   → Processing 53 words...\n",
      "Chunk 149/205 (size 74)\n",
      "   → Processing 74 words...\n",
      "Chunk 150/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 151/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 152/205 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 153/205 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 154/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 155/205 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 156/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 157/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 158/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 159/205 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 160/205 (size 28)\n",
      "   → Processing 28 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 161/205 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 162/205 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 163/205 (size 156)\n",
      "   → Processing 156 words...\n",
      "Chunk 164/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 165/205 (size 34)\n",
      "   → Processing 34 words...\n",
      "Chunk 166/205 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 167/205 (size 28)\n",
      "   → Processing 28 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 168/205 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 169/205 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 170/205 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 171/205 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 172/205 (size 55)\n",
      "   → Processing 55 words...\n",
      "Chunk 173/205 (size 94)\n",
      "   → Processing 94 words...\n",
      "Chunk 174/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 175/205 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 176/205 (size 94)\n",
      "   → Processing 94 words...\n",
      "Chunk 177/205 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 178/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 179/205 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 180/205 (size 65)\n",
      "   → Processing 65 words...\n",
      "Chunk 181/205 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 182/205 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 183/205 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 184/205 (size 53)\n",
      "   → Processing 53 words...\n",
      "Chunk 185/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 186/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 187/205 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 188/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 189/205 (size 24)\n",
      "   → Processing 24 words...\n",
      "Chunk 190/205 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 191/205 (size 23)\n",
      "   → Processing 23 words...\n",
      "Chunk 192/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 193/205 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 194/205 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 195/205 (size 26)\n",
      "   → Processing 26 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 196/205 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 197/205 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 198/205 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 199/205 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 200/205 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 201/205 (size 79)\n",
      "   → Processing 79 words...\n",
      "Chunk 202/205 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 203/205 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 204/205 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 205/205 (size 42)\n",
      "   → Processing 42 words...\n",
      "   Total speaker label changes: 96\n",
      "Saved → output_adaptive/SBC029.json\n",
      "\n",
      "Processing SBC045.json\n",
      "Adaptive chunking created 29 chunks.\n",
      "Chunk 1/29 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 2/29 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 3/29 (size 897)\n",
      "   → Processing 897 words...\n",
      "WARNING: transfer_llm_completion failed.\n",
      "Chunk 4/29 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 5/29 (size 638)\n",
      "   → Processing 638 words...\n",
      "Chunk 6/29 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 7/29 (size 764)\n",
      "   → Processing 764 words...\n",
      "Chunk 8/29 (size 27)\n",
      "   → Processing 27 words...\n",
      "Chunk 9/29 (size 59)\n",
      "   → Processing 59 words...\n",
      "Chunk 10/29 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 11/29 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 12/29 (size 21)\n",
      "   → Processing 21 words...\n",
      "Chunk 13/29 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 14/29 (size 25)\n",
      "   → Processing 25 words...\n",
      "Chunk 15/29 (size 67)\n",
      "   → Processing 67 words...\n",
      "Chunk 16/29 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 17/29 (size 46)\n",
      "   → Processing 46 words...\n",
      "Chunk 18/29 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 19/29 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 20/29 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 21/29 (size 313)\n",
      "   → Processing 313 words...\n",
      "Chunk 22/29 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 23/29 (size 66)\n",
      "   → Processing 66 words...\n",
      "Chunk 24/29 (size 229)\n",
      "   → Processing 229 words...\n",
      "Chunk 25/29 (size 137)\n",
      "   → Processing 137 words...\n",
      "Chunk 26/29 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 27/29 (size 69)\n",
      "   → Processing 69 words...\n",
      "Chunk 28/29 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 29/29 (size 752)\n",
      "   → Processing 752 words...\n",
      "   Total speaker label changes: 35\n",
      "Saved → output_adaptive/SBC045.json\n",
      "\n",
      "Processing SBC006.json\n",
      "Adaptive chunking created 29 chunks.\n",
      "Chunk 1/29 (size 353)\n",
      "   → Processing 353 words...\n",
      "Chunk 2/29 (size 24)\n",
      "   → Processing 24 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 3/29 (size 233)\n",
      "   → Processing 233 words...\n",
      "Chunk 4/29 (size 72)\n",
      "   → Processing 72 words...\n",
      "Chunk 5/29 (size 817)\n",
      "   → Processing 817 words...\n",
      "Chunk 6/29 (size 52)\n",
      "   → Processing 52 words...\n",
      "Chunk 7/29 (size 99)\n",
      "   → Processing 99 words...\n",
      "Chunk 8/29 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 9/29 (size 29)\n",
      "   → Processing 29 words...\n",
      "Chunk 10/29 (size 30)\n",
      "   → Processing 30 words...\n",
      "Chunk 11/29 (size 598)\n",
      "   → Processing 598 words...\n",
      "Chunk 12/29 (size 30)\n",
      "   → Processing 30 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 13/29 (size 39)\n",
      "   → Processing 39 words...\n",
      "Chunk 14/29 (size 35)\n",
      "   → Processing 35 words...\n",
      "Chunk 15/29 (size 1550)\n",
      "   → Processing 1550 words...\n",
      "Chunk 16/29 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 17/29 (size 203)\n",
      "   → Processing 203 words...\n",
      "Chunk 18/29 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 19/29 (size 306)\n",
      "   → Processing 306 words...\n",
      "Chunk 20/29 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 21/29 (size 88)\n",
      "   → Processing 88 words...\n",
      "Chunk 22/29 (size 19)\n",
      "   → Processing 19 words...\n",
      "Chunk 23/29 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 24/29 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 25/29 (size 312)\n",
      "   → Processing 312 words...\n",
      "Chunk 26/29 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 27/29 (size 366)\n",
      "   → Processing 366 words...\n",
      "Chunk 28/29 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 29/29 (size 137)\n",
      "   → Processing 137 words...\n",
      "   Total speaker label changes: 69\n",
      "Saved → output_adaptive/SBC006.json\n",
      "\n",
      "Processing SBC007.json\n",
      "Adaptive chunking created 75 chunks.\n",
      "Chunk 1/75 (size 33)\n",
      "   → Processing 33 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 2/75 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 3/75 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 4/75 (size 77)\n",
      "   → Processing 77 words...\n",
      "Chunk 5/75 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 6/75 (size 170)\n",
      "   → Processing 170 words...\n",
      "Chunk 7/75 (size 72)\n",
      "   → Processing 72 words...\n",
      "Chunk 8/75 (size 76)\n",
      "   → Processing 76 words...\n",
      "Chunk 9/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 10/75 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 11/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 12/75 (size 53)\n",
      "   → Processing 53 words...\n",
      "Chunk 13/75 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 14/75 (size 48)\n",
      "   → Processing 48 words...\n",
      "Chunk 15/75 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 16/75 (size 249)\n",
      "   → Processing 249 words...\n",
      "Chunk 17/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 18/75 (size 33)\n",
      "   → Processing 33 words...\n",
      "Chunk 19/75 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 20/75 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 21/75 (size 22)\n",
      "   → Processing 22 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 22/75 (size 141)\n",
      "   → Processing 141 words...\n",
      "Chunk 23/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 24/75 (size 31)\n",
      "   → Processing 31 words...\n",
      "Chunk 25/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 26/75 (size 19)\n",
      "   → Processing 19 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 27/75 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 28/75 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 29/75 (size 20)\n",
      "   → Processing 20 words...\n",
      "Chunk 30/75 (size 36)\n",
      "   → Processing 36 words...\n",
      "Chunk 31/75 (size 57)\n",
      "   → Processing 57 words...\n",
      "Chunk 32/75 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 33/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 34/75 (size 62)\n",
      "   → Processing 62 words...\n",
      "Chunk 35/75 (size 18)\n",
      "   → Processing 18 words...\n",
      "Chunk 36/75 (size 74)\n",
      "   → Processing 74 words...\n",
      "Chunk 37/75 (size 120)\n",
      "   → Processing 120 words...\n",
      "Chunk 38/75 (size 15)\n",
      "   → Processing 15 words...\n",
      "Chunk 39/75 (size 26)\n",
      "   → Processing 26 words...\n",
      "Chunk 40/75 (size 43)\n",
      "   → Processing 43 words...\n",
      "Chunk 41/75 (size 738)\n",
      "   → Processing 738 words...\n",
      "Chunk 42/75 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 43/75 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 44/75 (size 22)\n",
      "   → Processing 22 words...\n",
      "Chunk 45/75 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 46/75 (size 38)\n",
      "   → Processing 38 words...\n",
      "Chunk 47/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 48/75 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 49/75 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 50/75 (size 58)\n",
      "   → Processing 58 words...\n",
      "Chunk 51/75 (size 11)\n",
      "   → Processing 11 words...\n",
      "Chunk 52/75 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 53/75 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 54/75 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 55/75 (size 10)\n",
      "   → Processing 10 words...\n",
      "Chunk 56/75 (size 44)\n",
      "   → Processing 44 words...\n",
      "Chunk 57/75 (size 88)\n",
      "   → Processing 88 words...\n",
      "Chunk 58/75 (size 6)\n",
      "   → Processing 6 words...\n",
      "Chunk 59/75 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 60/75 (size 17)\n",
      "   → Processing 17 words...\n",
      "Chunk 61/75 (size 47)\n",
      "   → Processing 47 words...\n",
      "Chunk 62/75 (size 13)\n",
      "   → Processing 13 words...\n",
      "Chunk 63/75 (size 12)\n",
      "   → Processing 12 words...\n",
      "Chunk 64/75 (size 40)\n",
      "   → Processing 40 words...\n",
      "Chunk 65/75 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 66/75 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 67/75 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 68/75 (size 32)\n",
      "   → Processing 32 words...\n",
      "Chunk 69/75 (size 108)\n",
      "   → Processing 108 words...\n",
      "Skipping meaningless speaker token: <speaker:>\n",
      "Chunk 70/75 (size 8)\n",
      "   → Processing 8 words...\n",
      "Chunk 71/75 (size 16)\n",
      "   → Processing 16 words...\n",
      "Chunk 72/75 (size 14)\n",
      "   → Processing 14 words...\n",
      "Chunk 73/75 (size 7)\n",
      "   → Processing 7 words...\n",
      "Chunk 74/75 (size 9)\n",
      "   → Processing 9 words...\n",
      "Chunk 75/75 (size 7)\n",
      "   → Processing 7 words...\n",
      "   Total speaker label changes: 135\n",
      "Saved → output_adaptive/SBC007.json\n",
      "\n",
      "Completed: 17/17 files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from diarizationlm import utils\n",
    "\n",
    "def parse_word_level(json_data):\n",
    "    \"\"\"Convert the utterance into perfect 1:1 word entries.\"\"\"\n",
    "    if \"utterances\" not in json_data or len(json_data[\"utterances\"]) != 1:\n",
    "        raise ValueError(\"JSON must contain exactly one utterance\")\n",
    "\n",
    "    utt = json_data[\"utterances\"][0]\n",
    "\n",
    "    words = utt[\"hyp_text\"].split()\n",
    "    speakers = utt[\"hyp_spk\"].split()\n",
    "\n",
    "    if len(words) != len(speakers):\n",
    "        raise ValueError(\"Word/Speaker mismatch in original JSON\")\n",
    "\n",
    "    return [{\"word\": w, \"spk\": s, \"pos\": i} for i, (w, s) in enumerate(zip(words, speakers))]\n",
    "\n",
    "\n",
    "def chunk_adaptive(word_entries, overlap_window=5):\n",
    "    \"\"\"\n",
    "    ADAPTIVE STRATEGY:\n",
    "    - A chunk ends whenever a speaker switch happens.\n",
    "    - The next chunk *starts 5 words before* the switch (overlap).\n",
    "    - Allows reconciling speaker corrections in the transition area.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for i in range(1, len(word_entries)):\n",
    "        prev_spk = word_entries[i-1][\"spk\"]\n",
    "        curr_spk = word_entries[i][\"spk\"]\n",
    "\n",
    "        # Speaker change detected\n",
    "        if prev_spk != curr_spk:\n",
    "            end_idx = i  # chunk ends *before* the change word\n",
    "\n",
    "            chunk = word_entries[start_idx:end_idx]\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "            # Next chunk starts overlap_window words before the boundary\n",
    "            start_idx = max(0, i - overlap_window)\n",
    "\n",
    "    # Final chunk\n",
    "    if start_idx < len(word_entries):\n",
    "        chunks.append(word_entries[start_idx:])\n",
    "\n",
    "    print(f\"Adaptive chunking created {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def chunk_to_hypothesis_optimized(chunk):\n",
    "    \"\"\"Speaker-tag-only-on-change format.\"\"\"\n",
    "    if not chunk:\n",
    "        return \"\"\n",
    "\n",
    "    parts = []\n",
    "    current_speaker = None\n",
    "\n",
    "    for w in chunk:\n",
    "        if w[\"spk\"] != current_speaker:\n",
    "            current_speaker = w[\"spk\"]\n",
    "            parts.append(f\"<speaker:{current_speaker}>\")\n",
    "        parts.append(w[\"word\"])\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def process_chunk_with_diarizationlm(word_chunk, model, tokenizer):\n",
    "    hypothesis = chunk_to_hypothesis_optimized(word_chunk)\n",
    "    print(f\"   → Processing {len(word_chunk)} words...\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    inputs = tokenizer([hypothesis + \" --> \"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=min(800, int(inputs.input_ids.shape[1] * 1.1)),\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            num_beams=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    completion = tokenizer.batch_decode(\n",
    "        output[:, inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    try:\n",
    "        transferred = utils.transfer_llm_completion(completion, hypothesis)\n",
    "    except:\n",
    "        print(\"WARNING: transfer_llm_completion failed.\")\n",
    "        transferred = hypothesis\n",
    "\n",
    "    return transferred\n",
    "\n",
    "\n",
    "def parse_diarization_output_optimized(output, original_chunk):\n",
    "    pattern = r\"<speaker:(\\d+)>\\s*([^<]*)\"\n",
    "    matches = re.findall(pattern, output)\n",
    "\n",
    "    if not matches:\n",
    "        print(\"   WARNING: No speaker tags detected, fallback using original speakers.\")\n",
    "        return [w[\"spk\"] for w in original_chunk]\n",
    "\n",
    "    assigned = []\n",
    "    for spk, text in matches:\n",
    "        words = text.strip().split()\n",
    "        assigned.extend([spk] * len(words))\n",
    "\n",
    "    # Pad if model produced fewer words\n",
    "    if len(assigned) < len(original_chunk):\n",
    "        last_spk = assigned[-1] if assigned else original_chunk[0][\"spk\"]\n",
    "        assigned.extend([last_spk] * (len(original_chunk) - len(assigned)))\n",
    "\n",
    "    return assigned[:len(original_chunk)]\n",
    "\n",
    "\n",
    "def apply_chunk_speaker_updates(global_list, chunk, updated):\n",
    "    \"\"\"\n",
    "    Overwrites speaker labels in global words.\n",
    "    Later chunks have priority due to adaptive overlap.\n",
    "    \"\"\"\n",
    "    changes = 0\n",
    "    for w, new_spk in zip(chunk, updated):\n",
    "        if w[\"spk\"] != new_spk:\n",
    "            w[\"spk\"] = new_spk\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "\n",
    "\n",
    "def rebuild_json(global_words, original_json):\n",
    "    words = [x[\"word\"] for x in global_words]\n",
    "    speakers = [x[\"spk\"] for x in global_words]\n",
    "\n",
    "    utt = original_json[\"utterances\"][0].copy()\n",
    "    utt[\"hyp_text\"] = \" \".join(words)\n",
    "    utt[\"hyp_spk\"] = \" \".join(speakers)\n",
    "\n",
    "    out = original_json.copy()\n",
    "    out[\"utterances\"] = [utt]\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_single_json_file(json_file, model, tokenizer, output_folder):\n",
    "    print(f\"\\nProcessing {os.path.basename(json_file)}\")\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Parse words\n",
    "    words = parse_word_level(data)\n",
    "\n",
    "    # Adaptive chunking by speaker changes + overlap\n",
    "    chunks = chunk_adaptive(words, overlap_window=5)\n",
    "\n",
    "    global_words = words.copy()\n",
    "    total_changes = 0\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {idx+1}/{len(chunks)} (size {len(chunk)})\")\n",
    "\n",
    "        output = process_chunk_with_diarizationlm(chunk, model, tokenizer)\n",
    "        updated_speakers = parse_diarization_output_optimized(output, chunk)\n",
    "\n",
    "        # Overwrites earlier labels when overlaps occur\n",
    "        total_changes += apply_chunk_speaker_updates(global_words, chunk, updated_speakers)\n",
    "\n",
    "    print(f\"   Total speaker label changes: {total_changes}\")\n",
    "\n",
    "    # Rebuild JSON\n",
    "    output_json = rebuild_json(global_words, data)\n",
    "\n",
    "    out_path = os.path.join(output_folder, os.path.basename(json_file))\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(output_json, f, indent=2)\n",
    "\n",
    "    print(f\"Saved → {out_path}\")\n",
    "    return output_json\n",
    "\n",
    "\n",
    "def process_all_json_files(input_folder, output_folder):\n",
    "    print(\"Loading DiarizationLM model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    json_files = glob.glob(os.path.join(input_folder, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files.\\n\")\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    for jf in json_files:\n",
    "        if process_single_json_file(jf, model, tokenizer, output_folder) is not None:\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\nCompleted: {count}/{len(json_files)} files processed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_json_files(INPUT_FOLDER, OUTPUT_FOLDER)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "350252e872e245babc631a64c3859091": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a072c6dc2824cd191f818f423cfa140": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b9c936ea0c14dd8915bbcd851e8b47c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d1feb898cda4e499f59d9b10b7caf22",
       "IPY_MODEL_fd4e275826be46a98fdb920a2307d0ec",
       "IPY_MODEL_819233e7128d4793a8c2175516a559be"
      ],
      "layout": "IPY_MODEL_ca30465c6ba44a068a550aa19b4dccb3"
     }
    },
    "5d1feb898cda4e499f59d9b10b7caf22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dec216c0c773423d8b004b3435987f31",
      "placeholder": "​",
      "style": "IPY_MODEL_350252e872e245babc631a64c3859091",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "6b2dd57a54c34141a2c4697f2b3273bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "819233e7128d4793a8c2175516a559be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d447c0083a494b6893feb4ad7139f290",
      "placeholder": "​",
      "style": "IPY_MODEL_6b2dd57a54c34141a2c4697f2b3273bb",
      "value": " 4/4 [00:05&lt;00:00,  1.10s/it]"
     }
    },
    "ca30465c6ba44a068a550aa19b4dccb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d447c0083a494b6893feb4ad7139f290": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dec216c0c773423d8b004b3435987f31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f53cd412a3414ca2a6e386c47c094090": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd4e275826be46a98fdb920a2307d0ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f53cd412a3414ca2a6e386c47c094090",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a072c6dc2824cd191f818f423cfa140",
      "value": 4
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
